# -*- coding: utf-8 -*-
"""Cleaned Dataset + Deberta.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TowjBURF2GXjb0407Un022gGDaMa7bw-

# Import Libraries & Dataset
"""

# install and import libraries

!pip install transformers datasets accelerate -U
!pip install rapidfuzz
!pip install datasketch

import pandas as pd
import numpy as np
import re
import os
import torch
import nltk
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score, precision_score, recall_score

from datasets import Dataset, DatasetDict
from transformers import Trainer, TrainingArguments, DataCollatorWithPadding
from transformers import DebertaV2Tokenizer, DebertaV2ForSequenceClassification, AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, get_scheduler
from torch.utils.data import DataLoader
from torch.optim import AdamW
from accelerate import Accelerator
from tqdm.auto import tqdm
from nltk.corpus import stopwords
from datasketch import MinHash, MinHashLSH
import seaborn as sns
import matplotlib.pyplot as plt
import shutil

from google.colab import drive
from google.colab import files

# load datasets 'True.csv' and 'Fake.csv' from google drive and concat them into a single dataframe 'df'

drive.mount('/content/drive')
for f in os.listdir():
    print(f)

df_false = pd.read_csv('/content/drive/MyDrive/Fake.csv')
df_true = pd.read_csv('/content/drive/MyDrive/True.csv')

df_false["label"] = 0
df_true["label"] = 1

df = pd.concat([df_true, df_false], axis=0).reset_index(drop=True)

"""## Analyze & Clean Data"""

# analyze shape and label

print("Dataset Shape:", df.shape)
print("\nLabel Counts:")
print(df["label"].value_counts())

plt.figure(figsize=(5,4))
sns.countplot(data=df, x="label")
plt.title("Label Distribution (0 = Fake, 1 = True)")
plt.show()

# analyze sample content

print("ðŸ“° True articles:")
print(df[df["label"] == 1].head(10))

print("\nðŸ§¾ False articles:")
print(df[df["label"] == 0].head(10))

# check rows for any missing values

print(df[df.isna().any(axis=1)])

# print the unique values & their counts for column 'subject'

subject_label_counts = df.groupby(['subject', 'label']).size()
print(subject_label_counts)

# drop 'left-news' in Fake column as it doesn't have an equivalent in True columns and could learn bias

df = df[df['subject'] != 'left-news']
print(df['subject'].value_counts())

# drop 'subject' and 'date' columns as they don't carry significant information

df = df.drop(columns=['subject', 'date'])
print(df.columns.tolist())

# convert text in columns 'text' and 'title' to lowercase

df['text'] = df['text'].str.lower()
df['title'] = df['title'].str.lower()

# find word count metrics for text and title based on label

df['text_length'] = df['text'].apply(lambda x: len(str(x).split()))
df['title_length'] = df['title'].apply(lambda x: len(str(x).split()))

print(df.groupby('label')['text_length'].describe())
print(df.groupby('label')['title_length'].describe())

# remove any articles that only contain whitespaces
df = df[df['text'].str.strip() != '']
df = df[df['title'].str.strip() != '']

# remove outlier articles (based on word count)

# obtain q1 and q3 text lengths
q1_text = df['text_length'].quantile(0.25)
q3_text = df['text_length'].quantile(0.75)
iqr_text = q3_text - q1_text

q1_title = df['title_length'].quantile(0.25)
q3_title = df['title_length'].quantile(0.75)
iqr_title = q3_title - q1_title

# keep only articles within 1.5*IQR from q1 and q3
df = df[(df['text_length'] >= q1_text - 1.5*iqr_text) & (df['text_length'] <= q3_text + 1.5*iqr_text)]
df = df[(df['title_length'] >= q1_title - 1.5*iqr_title) & (df['title_length'] <= q3_title + 1.5*iqr_title)]

# remove any articles with less than 50 words in text
df = df[(df['text_length'] >= 50)]

# analyze change

print(df.groupby('label')['text_length'].describe())
print(df.groupby('label')['title_length'].describe())

# download stopwords from library
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))


# clean text function
def full_clean_text(text):
    if not isinstance(text, str):
        return ""

    # remove anything before "(Reuters) - "
    text = re.sub(r"^.*?(?:\(Reuters\)|Reuters)\s*-\s*", "", text, flags=re.IGNORECASE)

    # fix encoding artifacts
    try:
        text = text.encode('latin1').decode('utf-8')
    except:
        pass

    # remove URLs
    text = re.sub(r'http\S+|www\.\S+', '', text)

    # remove month names and numbers
    text = re.sub(
    r'\b(?:jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec|'
    r'january|february|march|april|may|june|july|august|september|october|november|december)\b\.?',
    '',
    text,
    flags=re.IGNORECASE
    )
    text = re.sub(r'\d+', '', text)


    # remove stopwords
    words = text.split()
    words = [w for w in words if w.lower() not in stop_words]
    text = " ".join(words)

    # remove weird symbols
    text = re.sub(r'[^a-zA-Z0-9\s.,!?\'"-]', ' ', text)

    # get rid of multiple whitespaces
    text = re.sub(r'\s+', ' ', text).strip()
    return text


# apply to both text and title
df['text'] = df['text'].apply(full_clean_text)
df['title'] = df['title'].apply(full_clean_text)

# print column count prior to duplicate and near duplicate removal
print(df.shape)

# remove duplicates
df = df.drop_duplicates(subset=["text"]).reset_index(drop=True)


# break text into shingles

shingle_size = 7                    # num of words per shingle
num_perm = 256                      # num of permutations for MinHash
similarity_threshold = 0.85         # MinHash similarity threshold for near-duplicates

def get_shingles(text, k=shingle_size):
    words = text.split()
    shingles = set()
    for i in range(max(len(words) - k + 1, 1)):
        shingles.add(" ".join(words[i:i+k]))
    return shingles


# remove near duplicates
def remove_near_duplicates(df, text_column="text"):
    lsh = MinHashLSH(threshold=similarity_threshold, num_perm=num_perm)
    minhashes = {}
    for idx, text in enumerate(df[text_column]):
        shingles = get_shingles(text)
        m = MinHash(num_perm=num_perm)
        for shingle in shingles:
            m.update(shingle.encode('utf8'))
        minhashes[idx] = m
        lsh.insert(idx, m)
    to_drop = set()
    for idx, m in minhashes.items():
        if idx in to_drop:
            continue
        result = lsh.query(m)
        for j in result:
            if j != idx:
                to_drop.add(j)
    df_cleaned = df.drop(list(to_drop)).reset_index(drop=True)
    return df_cleaned


# update df to clean ver
df = remove_near_duplicates(df, text_column="text")


# print column count after duplicate and near duplicate cleaning
print(df.shape)

# Count how many articles are true and fake
print(df['label'].value_counts())

# print samples of current state of dataset
print("ðŸ“° True articles:")
print(df[df["label"] == 1].head(10))

print("\nðŸ§¾ False articles:")
print(df[df["label"] == 0].head(10))

# combine text and title columns into one
df['title_and_text'] = " [TITLE] " + df['title'] + " [ARTICLE] " + df['text']

X = df['title_and_text']
y = df['label']

# drop all other columns besides 'title_and_text' and 'label'
df.drop(columns = ['title', 'text', 'text_length', 'title_length'], inplace=True)

# download clean dataset as csv file

from google.colab import files
df.to_csv('cleaned_articles.csv', index=False)
files.download('cleaned_articles.csv')

"""## TRAINING DATA"""

# load model
model_name = "microsoft/deberta-v3-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)

# create a hugging face dataset

ds = Dataset.from_pandas(df)
ds = ds.rename_column("label", "labels")

# tokenize function

def tokenize_function(x):
  return tokenizer(x['title_and_text'], padding='max_length', truncation=True, max_length=512)

tokenized_ds = ds.map(tokenize_function, batched=True)

# split tokenized dataset into training and test set

dds = tokenized_ds.train_test_split(test_size=0.25, seed=123)
train_ds = dds['train']
eval_ds = dds['test']

# declare and initialize hyperparameters

batch_size = 16
epochs = 1
learning_rate = 1e-5

# set training configuration

args = TrainingArguments(
    output_dir='outputs',
    learning_rate=learning_rate,
    warmup_ratio=0.1,
    lr_scheduler_type='cosine',
    fp16=True,
    eval_strategy='epoch',
    per_device_train_batch_size=batch_size,
    per_device_eval_batch_size=batch_size*2,
    num_train_epochs=epochs,
    weight_decay=0.3,
    report_to='none',
)

# function for computing evaluation metrics

def compute_metrics(eval_pred):
    predictions, labels = eval_pred

    # turn model outputs into class predictions and probabilities
    if predictions.ndim > 1 and predictions.shape[1] > 1:
        exp_preds = np.exp(predictions)
        probs = exp_preds / np.sum(exp_preds, axis=1, keepdims=True)
        preds = np.argmax(predictions, axis=1)
        pos_probs = probs[:, 1]
    else:
        preds = (predictions > 0.5).astype(int)
        pos_probs = predictions

    # compute metrics
    accuracy = accuracy_score(labels, preds)
    f1 = f1_score(labels, preds)
    precision = precision_score(labels, preds)
    recall = recall_score(labels, preds)

    try:
        roc_auc = roc_auc_score(labels, pos_probs)
    except ValueError:
        roc_auc = float('nan')

    return {
        "accuracy": accuracy,
        "f1": f1,
        "precision": precision,
        "recall": recall,
        "roc_auc": roc_auc
    }

# train model

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_ds,
    eval_dataset=eval_ds,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics
)

trainer.train()

# get metrics from trainer

metrics = trainer.evaluate()
print(metrics)

"""# **Trust Score Calculation**"""

pred_output = trainer.predict(eval_ds)
logits = pred_output.predictions
true_labels = pred_output.label_ids

probs = torch.softmax(torch.tensor(logits), dim=-1).numpy()
trust_scores = probs[:, 1]              # P(label=1)
pred_labels = np.argmax(logits, axis=1)
trust_scores_rounded = np.round(trust_scores, 4)

deberta_predictions_df = pd.DataFrame({
    "True Label": true_labels,
    "Predicted Label": pred_labels,
    "Trust Score": trust_scores_rounded
})

print(deberta_predictions_df.head(10))

"""# **Visuals & Evals**"""

# get predictions for plotting

pred_output = trainer.predict(eval_ds)
pred_logits = pred_output.predictions
true_labels = pred_output.label_ids

# softmax to get probabilities
probs = torch.softmax(torch.tensor(pred_logits), dim=1).numpy()
pos_probs = probs[:, 1]

# convert logits â†’ predicted class
pred_classes = np.argmax(pred_logits, axis=1)

# metrics already computed
print("Eval Metrics:", metrics)

# accuracy, precision, recall, f1

# extract metrics from trainer.evaluate()
accuracy = metrics["eval_accuracy"]
precision = metrics["eval_precision"]
recall = metrics["eval_recall"]
f1 = metrics["eval_f1"]

# metric names and values
metric_names = ["Accuracy", "Precision", "Recall", "F1-Score"]
metric_values = [accuracy, precision, recall, f1]

colors = sns.color_palette("Blues", len(metric_values))

# plot
plt.figure(figsize=(10, 6))
sns.set_style("whitegrid")

bars = plt.bar(metric_names, metric_values, color=colors)

# add value labels above each bar
for bar, value in zip(bars, metric_values):
    plt.text(
        bar.get_x() + bar.get_width() / 2,
        bar.get_height() + 0.005,
        f"{value:.3f}",
        ha='center',
        va='bottom',
        fontsize=12,
        fontweight='bold'
    )

# title and labels
plt.title("Model Performance Metrics", fontsize=18, fontweight='bold')
plt.ylabel("Score", fontsize=14)
plt.xlabel("Metric", fontsize=14)
plt.ylim(0, 1.05)

plt.show()

# confusion matrix
cm = confusion_matrix(true_labels, pred_classes)

plt.figure(figsize=(6,5))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=["False", "True"],
            yticklabels=["False", "True"])
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

# roc curve

fpr, tpr, thresholds = roc_curve(true_labels, pos_probs)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f"AUC = {roc_auc:.3f}")
plt.plot([0,1], [0,1], linestyle="--")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend()
plt.show()

from google.colab import drive
drive.mount('/content/drive')

# saving the finetuned model and tokenizer

# define the path where the finetuned model and tokenizer will be saved
clean_path = "/content/drive/MyDrive/my_finetuned_deberta_clean"

# create the directory if it doesn't exist
if not os.path.exists(clean_path):
    os.makedirs(clean_path)

# save the finetuned model using the trainer
# the 'trainer' and 'tokenizer' objects are available from previous cells after training.
trainer.save_model(clean_path)
tokenizer.save_pretrained(clean_path)

print(f"Saved finetuned model and tokenizer to: {clean_path}")

os.listdir("/content/drive/MyDrive/my_finetuned_deberta_clean")
